# Recon Everything

Bug Bounty Hunting Tip #1- Always read the Source Code

## Approach a Target (Lot of this section is taken from 

[Jason Haddix](https://medium.com/u/1dfc5adea2d4?source=post_page-----48aafbb8987--------------------------------)

 and portswigger blog)

â€¢ Ideally youâ€™re going to be wanting to choose a program that has a wide scope. Youâ€™re also going to be wanting to look for a bounty program that has a wider range of vulnerabilities within scope.

â€¢ Mining information about the domains, email servers and social network connections.

â€¢ Dig in to website, check each request and response and analyse that, try to understand their infrastructure such as how theyâ€™re handling sessions/authentication, what type of CSRF protection they have (if any).

â€¢ Use negative testing to through the error, this Error information is very helpful for me to finding internal paths of the website. Give time to understand the flow of the application to get a better idea of what type of vulnerabilities to look for.

â€¢ Start to dig into using scripts for wordlist bruteforcing endpoints. This can help with finding new directories or folders that you may not have been able to find just using the website.  

This tends to be private admin panels, source repositories they forgot to remove such as /.git/ folders, or test/debug scripts. After that check each form of the website then try to push client side attacks. Use multiple payloads to bypass client side filters.

â€¢ Start early. As soon as a program is launched, start hunting immediately, if you can.

â€¢ Once you start hunting, take a particular functionality/workflow in the application and start digging deep into it. I have stopped caring about low hanging fruits or surface bugs. There is no point focusing your efforts on those.

â€¢ So, letâ€™s say an application has a functionality that allows users to send emails to other users.

â€¢ Observe this workflow/requests via a proxy tool such as Burp. Burp is pretty much the only tool I use for web app pen testing.

â€¢ Create multiple accounts because you would want to test the emails being sent from one user to another. If you havenâ€™t been provided multiple accounts, ask for it. Till date, I have not been refused a second account whenever I have asked for it.

â€¢ Now, if you are slightly experienced, after a few minutes of tinkering with this workflow, you will get a feeling whether it might have something interesting going on or not. This point is difficult to explain. It will come with practice.

â€¢ If the above is true, start fuzzing, breaking the application workflow, inserting random IDs, values, etc. wherever possible. 80% of the time, you will end up noticing weird behavior.

â€¢ The weird behavior doesnâ€™t necessarily mean you have found a bug that is worth reporting. It probably means you have a good chance so you should keep digging into it more.

â€¢ There is some research that might be required as well. Letâ€™s say you found that a particular version of an email server is being used that is outdated. Look on the internet for known vulnerabilities against it. You might encounter a known CVE with a known exploit. Try that exploit and see what happens (provided you are operating under the terms and conditions of the bug bounty).

â€¢ There might be special tools that are required. Explore into that, if possible. Remember, Burp is a swiss army knife but you might have to use certain specific tools in certain cases. Always, be aware of that.

â€¢ After spending a few hours on this, if you think you have exhausted all your options and are not getting anything meaningful out of it, stop and move on. Getting hung up on something is the biggest motivation killer but that doesnâ€™t mean you are giving up. Get back to it later if something else comes up. Make a note of it.

â€¢ Something that has worked for me is bounds checking on parameters, pick a parameter that has an obvious effect on the flow of the application.  

For example, if a field takes a number (lets call it ID for lulz).  

What happens if:  

\-you put in a minus number?  

\-you increment or decrement the number?  

\-you put in a really large number?  

\-you put in a string or symbol characters?  

\-you try traverse a directory with â€¦/  

\-you put in XSS vectors?  

\-you put in SQLI vectors?  

\-you put in non-ascii characters?  

\-you mess with the variable type such as casting a string to an array  

\-you use null characters or no value

I would then see if I can draw any conclusions from the outcomes of these tests,  

\-see if I can understand what is happening based on an error  

\-is anything broken or exposed  

\-can this action affect other things in the app.

â€¢ Focus on site functionality that has been redesigned or changed since a previous version of the target. Sometimes, having seen/used a bounty product before, you will notice right away any new  

functionality. Other times you will read the bounty brief a few times and realize that they are giving you a map. Developers often point out the areas they think they are weak in. They/us want you to  

succeed. A visual example would be new search functionality, role based access, etc. A bounty brief example would be reading a brief and noticing a lot of pointed references to the API or a particular page/function in the site.

â€¢ If the scope allows (and you have the skillset) test the crap out of the mobile apps. While client side bugs continue to grow less severe, the APIâ€™s/web-endpoints the mobile apps talk to often touch parts of the application you wouldnâ€™t have seen in a regular workflow. This is not to say client side bugs are not reportable, they just become low severity issues as the mobile OSâ€™s raise the bar security-wise.

â€¢ So after you have a thorough â€œfeelingâ€ for the site you need to mentally or physically keep a record of workflows in the application. You need to start asking yourself questions like these:

â€¢ Does the page functionality display something to the users? (XSS,Content Spoofing, etc)

â€¢ Does the page look like it might need to call on stored data?

â€¢ (Injections of all type, Indirect object references, client side storage)

â€¢ Does it (or can it) interact with the server file system? (Fileupload vulns, LFI, etc)

â€¢ Is it a function worthy of securing? (CSRF, Mixed-mode)

â€¢ Is this function a privileged one? (logic flaws, IDORs, priv escalations)++

â€¢ Where is input accepted and potentially displayed to the user?

â€¢ What endpoints save data?

â€¢ Any file upload functionality?

â€¢ What type of authentication is used?

## Steps to take when approaching a target

**Walk through the application**

The first step to identifying vulnerabilities in a web application is actually using the web application. Use the web application like the actual user would:

**create an account** click on the links you can see what the application does(and to identify an attack surface i.e. what parts of the application have functionality that you can attack) use the different functionality(e.g. making transactions) Through this it is important to identify common themes such as:

What languages/frameworks did the developer use to create the application. What version of the server/language did the developer use(if specified in the application) During the walk through, itâ€™s important to think like a developer. During this process try and think of the design/implementation of a particular feature, and using these features in a way that the developer did not intend for them to be used.

1\. Check/Verify targetâ€™s scope (\*.example.com)

2\. Find subdomains of target (Refer Subdomain tools mentioned in the article)

3\. Run masscan

4\. Check which domains resolve

5\. Take Screenshot

6\. Do Content Discovery (by bruteforcing the files and directories on a particular domain/subdomain)

**Web Tools:**  

[https://pentest-tools.com/](https://pentest-tools.com/)  

[https://virustotal.com/](https://virustotal.com/)  

[https://www.shodan.io/](https://www.shodan.io/)  

[https://crt.sh/?q=%25target.com](https://crt.sh/?q=%25target.com)  

[https://dnsdumpster.com/](https://dnsdumpster.com/)  

[https://censys.io](https://censys.io/)  

[http://dnsgoodies.com](http://dnsgoodies.com/)

# Recon

â€¢ Recon shouldnâ€™t just be limited to finding assets and outdated stuff. Itâ€™s also understanding the app and finding functionality thatâ€™s not easily accessible. There needs to be a balance between recon and good old hacking on the application in order to be successful â€” @NahamSec

**Subdomain Enumeration Tools:**

-   It is recommended to go through the github links for usage of tools.

â€¢ **Enumerating Domains (Note: wherever you see bigdash( â€” ) below these are actually two dashes together(- -), medium post converted two dashes together with space with a single big dash)**

a. Vertical domain corelation (all the subdomain of a domain) (maps.google.com) â†’ Any subdomain of a particular base domain  

b. Horizontal domain corelation ( like google.com, google.cz, youtube.com, blogger.com) â†’ anything that is acquired by Google as entity.

1. **Sublist3r** â€” [https://github.com/aboul3la/Sublist3r](https://github.com/aboul3la/Sublist3r)

**Setup:**

> git clone [https://github.com/aboul3la/Sublist3r.git](https://github.com/aboul3la/Sublist3r.git)  

> sudo pip install -r requirements.txt

**Usage:**

â€“ To enumerate subdomains of specific domain:

> python sublist3r.py -d example.com

**Alias:**

> alias sublist3r=â€™python /path/to/Sublist3r/sublist3r.py -d â€˜

> 

> alias sublist3r-one=â€. <(cat domains | awk â€˜{print â€œsublist3r â€œ$1 â€œ -o â€œ $1 â€œ.txtâ€}â€™)â€

2. **subfinder** â€” [https://github.com/subfinder/subfinder](https://github.com/subfinder/subfinder)

**Setup:**

> go get github.com/subfinder/subfinder

**Usage:**

> subfinder -d freelancer.com

> 

> ./subfinder -dL hosts.txt

To find domain recursively:

> subfinder -d <domain> -recursive -silent -t 200 -v -o <outfile>

For using bruteforcing capabilities, you can use -b flag with -w option to specify a wordlist.

> ./subfinder -d freelancer.com -b -w jhaddix\_all.txt -t 100 â€” sources censys â€” set-settings CensysPages=2 -v

The -o command can be used to specify an output file.

3. **findomain** â€” [https://github.com/Edu4rdSHL/findomain](https://github.com/Edu4rdSHL/findomain)

You can monitor the subdomains and provide the webhooks to get notifications on Slack and discord.

**Setup:**

> $ wget [https://github.com/Edu4rdSHL/findomain/releases/latest/download/findomain-linux](https://github.com/Edu4rdSHL/findomain/releases/latest/download/findomain-linux)  

> $ chmod +x findomain-linux

**Usage:**

> findomain -t example.com

4. **assetfinder â€” **[https://github.com/tomnomnom/assetfinder](https://github.com/tomnomnom/assetfinder)

â€“ Find domains and subdomains potentially related to a given domain.

**Setup:**

> go get -u github.com/tomnomnom/assetfinder

**Usage:**

> assetfinder -subs-only <domain>

> 

> cat domains | assetfinder -subs-only ( make sure domains file is without http:// or [https://)](https://%29/)

5. **Amass: **[https://github.com/OWASP/Amass](https://github.com/OWASP/Amass)

**Setup:**

> go get -u github.com/OWASP/Amass/â€¦

**Usage:**

> amass enum -o subdomains.txt -d output\_file.txt

or

> amass enum -o out.txt -df domains.txt

All discovered domains are run through reverse whois(horizontal subdomain enum)

> amass intel -whois -d example.com

6. **censys-enumeration** â€” [https://github.com/0xbharath/censys-enumeration](https://github.com/0xbharath/censys-enumeration)

â€“ This is the most important steps, because the subdomains names that you find here, you cannot find from other bruteforce tools because your wordlist does not have pattern that are available in all the subdomains or does not have keyword like gateway or payment which are part of subdomain.

> search query â€” 443.https.tls.certificate.parsed.extensions.subject\_alt\_name.dns\_names:snapchat.com

A script to extract subdomains/emails for a given domain using SSL/TLS certificates dataset on Censys

**Setup:**

â€“ Clone this repo

> $ git clone git@github.com:yamakira/censys-enumeration.git

â€“ Install dependencies

> $ pip install -r requirements.txt

â€“ Get Censys API ID and Censys API secret by creating a account on [https://censys.io](https://censys.io/)

â€“ Add Censys API ID and Censys API secret as CENSYS\_API\_ID & CENSYS\_API\_SECRET respectively to the OS environment variables. On Linux you can use a command similar to following to do this

> $ export CENSYS\_API\_SECRET=â€iySd1n0l2JLnHTMisbFHzxClFuE0"

**Usage:**

> $ python censys\_enumeration.py â€” no-emails â€” verbose â€” outfile results.json domains.txt

7. **altdns** â€” [https://github.com/infosec-au/altdns](https://github.com/infosec-au/altdns)

â€“ It generates the possible combinations of original domain with the *words* from the wordlist ([*example*](https://github.com/infosec-au/altdns/blob/master/words.txt)).

**Setup:**

> pip install py-altdns

**Usage:**

> \# python altdns.py -i input\_domains.txt -o ./output/path -w altdns/words.txt -i subdomains.txt -o data\_output -w words.txt -s results\_output.txt

8. **Massdns:** [https://github.com/blechschmidt/massdns](https://github.com/blechschmidt/massdns)

**Setup:**

> git clone [https://github.com/blechschmidt/massdns.git](https://github.com/blechschmidt/massdns.git)  

> cd massdns  

> make

**Usage:**

> ./bin/massdns \[options\] \[domainlist\]

Resolve all A records from domains within domains.txt using the resolvers within resolvers.txt in lists and store the results within results.txt:

> $ ./bin/massdns -r lists/resolvers.txt -t A domains.txt > results.txt

9. **domains-from-csp** â€” [https://github.com/0xbharath/domains-from-csp](https://github.com/0xbharath/domains-from-csp)

â€“ Content-Security-Policy header allows us to create a whitelist of sources of trusted content, and instructs the browser to only execute or render resources from those domains(sources).

**Setup:**

> $ git clone git@github.com:yamakira/censys-enumeration.git  

> $ pipenv install

**Usage:**

> \# python csp\_parser.py target\_url  

> \# python csp\_parser.py target\_url â€” resolve

10. **Using SPF record of DNS** â€” [https://github.com/0xbharath/assets-from-spf/](https://github.com/0xbharath/assets-from-spf/)

â€“ A Python script to parse netblocks & domain names from SPF(Sender Policy Framework) DNS record

â€“ For every parsed asset, the script will also find and print Autonomous System Number(ASN) details

**Setup:**

> $ git clone git@github.com:yamakira/assets-from-spf.git  

> $ pipenv install

**Usage:**

â€“ Parse the SPF record for assets but donâ€™t do ASN enumeration

> $ python assets\_from\_spf.py target\_url

â€“ Parse the SPF record for assets and do ASN enumeration

> $ python assets\_from\_spf.py target\_url â€” asn

**Get ASN Number:**

â€“ Autonomous System Number (ASN) -> [http://bgp.he.net](http://bgp.he.net/) -> check for example tesla.com and checkin Prefixes V4 to get the IP range

or

> $ curl -s [http://ip-api.com/json/192.30.253.113](http://ip-api.com/json/192.30.253.113) | jq -r .as

AS36459 GitHub, Inc.

â€“ The ASN numbers found can be used to find netblocks of the domain.

â€“ We can use advanced WHOIS queries to find all the IP ranges that belong to an ASN

> $ whois -h whois.radb.net â€” â€˜-i origin AS36459â€™ | grep -Eo â€œ(\[0â€“9.\]+){4}/\[0â€“9\]+â€ | uniq

There is an Nmap script to find IP ranges that belong to an ASN that  

[https://nmap.org/nsedoc/scripts/targets-asn.html](https://nmap.org/nsedoc/scripts/targets-asn.html)

> $ nmap â€” script targets-asn â€” script-args targets-asn.asn=17012 > paypal.txt

Clean up the output from the above nmap result, take all the IPs in a file and then run version scanning on them or masscan on them.

> nmap -p- -sV -iL paypal.txt -oX paypal.xml

â€“ you can use dig

> $ dig AXFR @<nameserver> <domain\_name>

11. **Certspotter** â€” [https://certspotter.com/api/v0/certs?domain=hackerone.com](https://certspotter.com/api/v0/certs?domain=hackerone.com)

â€“ Good for vertical and horizontal corelation

â€“ you can get domain names, subdomain names

â€“ email address in a certificate

find-cert() {  

  

  curl -s https://certspotter.com/api/v0/certs?domain=$1 | jq -c '.\[\].dns\_names' | grep -o '"\[^"\\+"';  

}

12. **Crt.sh** â€” [https://crt.sh/?q=%25domain.com](https://crt.sh/?q=%25domain.com)

13. **knockpy **â€” [https://github.com/guelfoweb/knock.git](https://github.com/guelfoweb/knock.git)

**Setup:**

> $ sudo apt-get install python-dnspython  

> $ git clone [https://github.com/guelfoweb/knock.git](https://github.com/guelfoweb/knock.git)  

> Set your virustotal API\_KEY:  

> $ nano knockpy/config.json  

> $ sudo python setup.py install

**Usage:**

> $ knockpy domain.com -w wordlist.txt

14. **Shodan** -

Ports:8443, 8080  

Title: â€œDashboard\[Jenkins\]â€  

Product: Tomcat  

Hostname: example.com  

Org: google  

ssl:Google

To find jenkins instance in a target:

org:{org name;x-jenkins:200}

15. **Viewdns.info (Horizontal Domain Enumeration) Reverse whois lookup** â€” if you know the â€œemail id â€œ in the registrar of a domain and you want to check what other domains are registered with the same email id you can use this site. **Most of the tools does not find Horizontal Domain Enumeration.**

Get email address using â€” $ whois <domain.com>

or get the email and input in this website : [https://tools.whoisxmlapi.com/reverse-whois-search](https://tools.whoisxmlapi.com/reverse-whois-search)

I found that this site gives more domains than viewdns.info

Also it has option to export result in CSV.

16. **Sublert â€” **[https://github.com/yassineaboukir/sublert](https://github.com/yassineaboukir/sublert)

â€¢ Sublert is a security and reconnaissance tool which leverages certificate transparency to automatically monitor new subdomains deployed by specific organizations and issued TLS/SSL certificate.

**Setup:** [https://medium.com/@yassineaboukir/automated-monitoring-of-subdomains-for-fun-and-profit-release-of-sublert-634cfc5d7708](https://medium.com/@yassineaboukir/automated-monitoring-of-subdomains-for-fun-and-profit-release-of-sublert-634cfc5d7708)

> $ git clone [https://github.com/yassineaboukir/sublert.git](https://github.com/yassineaboukir/sublert.git) && cd sublert  

> $ sudo pip3 install -r requirements.txt

**Usage:**

Letâ€™s add PayPal for instance:

> $ python sublert.py -u paypal.com

Letâ€™s make Sublert.py executable:

> $ chmod u+x sublert.py

Now, we need to add a new Cron job to schedule execution of Sublert at given time. To do it, type:

> $ Crontab -e

Add the following line at the end of the Cron file:

> 0 \*/12 \* \* \* cd /root/sublert/ && /usr/bin/python3 sublert.py -r -l >> /root/sublert/sublert.log 2>&1

**Jason Haddix** (https://twitter.com/jhaddix/status/972926512595746816?lang=en)  

The lost art of LINKED target discovery w/ Burp Suite:  

1) Turn off passive scanning  

2) Set forms auto to submit  

3) Set scope to advanced control and use string of target name (not a normal FQDN)  

4) Walk+browse, then spider all hosts recursively!  

5) Profit (more targets)!

# Content Discovery Tools (Directory Bruteforcing)

â€¢ Use robots.txt to determine the directories.

â€¢ Also spider the host for API endpoints.

â€¢ you see an open port on 8443

â€¢ Directory brute force

â€¢ /admin/ return 403

â€¢ You bruteforce for more files/direcotries on /admin/

â€¢ and letâ€™s say /admin/users.php return 200

â€¢ Repeat on other domain, ports, folders etc

1. **ffuf** â€” [https://github.com/ffuf/ffuf](https://github.com/ffuf/ffuf)

â€“ A fast web fuzzer written in Go.

**Setup:**

> go get github.com/ffuf/ffuf

**Usage:**

-   **Typical directory discovery:**

> ffuf -w /path/to/wordlist -u [https://target/FUZZ](https://target/FUZZ)

**Test a wordlist through several hosts:**

> ffuf -u [https://HFUZZ/WFUZZ](https://hfuzz/WFUZZ) -w hosts.txt:HFUZZ -w wordlist.txt:WFUZZ -mode clusterbomb

-   Have your hosts list in a separate wordlist, as ffuf now supports multiple wordlists (and keywords)

> ffuf -w hosts.txt:HOSTS -w content.txt:FUZZ -u [https://HOSTS/FUZZ](https://hosts/FUZZ)  

>   

> ffuf -u url/FUZZ/FUZZ/FUZZ -w wordlist1.txt -w wordlist2.txt -w wordlist3.txt -mode clusterbomb

Then it will perform wordlist1 wordlist2 wordlist3 requests.

**Virtual host discovery (without DNS records)**

\- **First figure out the response length of false positive.**

> curl -s -H â€œHost: nonexistent.example.comâ€ [http://example.com](http://example.com/) | wc -c

> 

> ffuf -c -w /path/to/wordlist -u [http://example.com](http://example.com/) -H â€œHost: FUZZ.example.comâ€ -fs <length\_of\_flase\_positive>

**GET parameter fuzzing**  

GET parameter name fuzzing is very similar to directory discovery, and works by defining the FUZZ keyword as a part of the URL. This also assumes an response size of 4242 bytes for invalid GET parameter name.

> ffuf -w /path/to/paramnames.txt -u [https://target/script.php?FUZZ=test\_value](https://target/script.php?FUZZ=test_value) -fs 4242

If the parameter name is known, the values can be fuzzed the same way. This example assumes a wrong parameter value returning HTTP response code 401.

> ffuf -w /path/to/values.txt -u [https://target/script.php?valid\_name=FUZZ](https://target/script.php?valid_name=FUZZ) -fc 401

example:  

ffuf -w params.txt:HFUZZ -H â€œCookie: \_\_cfduid=d12ff6c4c7-s 915d707ec42577f244631577769628; 1337session=6430373065623634333431343933â€ -u [http://bugbountylab.art/target/web/recon/paramining-1/?HFUZZ=VFUZZ](http://bugbountylab.art/target/web/recon/paramining-1/?HFUZZ=VFUZZ) -w value.txt:VFUZZ -fs 31 -t 150

-   **POST data fuzzing**  

    This is a very straightforward operation, again by using the FUZZ keyword. This example is fuzzing only part of the POST request. Weâ€™re again filtering out the 401 responses.

> ffuf -w /path/to/postdata.txt -X POST -d â€œusername=admin\\&password=FUZZâ€ -u [https://target/login.php](https://target/login.php) -fc 401

2. **dirsearch **â€” [https://github.com/maurosoria/dirsearch.git](https://github.com/maurosoria/dirsearch.git) or [https://github.com/Damian89/dirsearch](https://github.com/Damian89/dirsearch)

**Setup:**

> git clone [https://github.com/Damian89/dirsearch.git](https://github.com/Damian89/dirsearch.git)  

> cd dirsearch  

> python3 dirsearch.py -u <URL> -e <EXTENSION>  

> python3 dirsearch.py -e php,txt,zip -u [https://target](https://target/) -w db/dicc.txt â€” recursive -R 2

**Wordlist:**

> $ wget [https://gist.githubusercontent.com/EdOverflow/c4d6d8c43b315546892aa5dab67fdd6c/raw/7dc210b17d7742b46de340b824a0caa0f25cf3cc/open\\\_redirect\\\_wordlist.txt](https://gist.githubusercontent.com/EdOverflow/c4d6d8c43b315546892aa5dab67fdd6c/raw/7dc210b17d7742b46de340b824a0caa0f25cf3cc/open/_redirect/_wordlist.txt)

**Alias:**

> alias dirsearch=â€™python3 /path/to/dirsearch/dirsearch.py -u â€˜  

> alias dirsearch-one=â€. <(cat domains | awk â€˜{print â€œdirsearch â€œ$1 â€œ -e \*â€}â€™)â€  

> alias openredirect=â€. <(cat domains | awk â€˜{print â€œdirsearch â€œ$1 â€œ -w /path/to/dirsearch/db/openredirectwordlist.txt -e \*â€}â€™)â€

3. **Gobuster** â€” [https://github.com/OJ/gobuster](https://github.com/OJ/gobuster)

**Setup:**

> go get github.com/OJ/gobuster

**Usage:**

> gobuster dir -u [https://mysite.com/path/to/folder](https://mysite.com/path/to/folder) -c â€˜session=123456â€™ -t 50 -w common-files.txt -x .php,.html

4. **wfuzz **â€” [https://github.com/xmendez/wfuzz/](https://github.com/xmendez/wfuzz/)

**Setup:**

> pip install wfuzz

**Usage( :**

> $ wfuzz -w raft-large-directories.txt â€” sc 200,403,302 http://testphp.vulnweb.com/FUZZ

5. **Burp Intruder**

**Screenshot Tools:**

â€¢ Look at the headers to see which security options are in place, for example looking for presence of X-XSS-Protection: or X-Frame-Options: deny.

â€¢ Knowing what security measures are in place means you know your limitations.

1. **Aquatone **â€” [https://github.com/michenriksen/aquatone](https://github.com/michenriksen/aquatone)

**Setup:**

> go get -u github.com/michenriksen/aquatone

**Usage:**

> cat hosts.txt | aquatone -out ~/aquatone/example.com

2. **Eyewitness:** [https://github.com/FortyNorthSecurity/EyeWitness](https://github.com/FortyNorthSecurity/EyeWitness)

**Setup:**

> $ git clone [https://github.com/FortyNorthSecurity/EyeWitness.git](https://github.com/FortyNorthSecurity/EyeWitness.git)

Navigate into the setup directory  

Run the setup.sh script

**Usage:**

> ./EyeWitness -f urls.txt â€” web  

>   

> ./EyeWitness -x urls.xml â€” timeout 8 â€” headless

3. **Webscreenshot:** [https://github.com/maaaaz/webscreenshot](https://github.com/maaaaz/webscreenshot)

**Setup:**

> $ apt-get update && apt-get install phantomjs  

> $ pip install webscreenshot

**Usage:**

> $ python webscreenshot.py -i list.txt -v

â€¢ Once this is done, we use a tool called epg-prep (https://www.npmjs.com/package/epg-prep) to create thumbnails to do so, simply run: epg-prep uber.com  

This will allow us to view the created pictures using express-photo-gallery.

â€¢ In a final step, use the express-gallery-script from the bottom of this blogpost and save it as yourname.js. All you need to do is to change the folder name inside the script: app.use(â€˜/photosâ€™, Gallery(â€˜uber.comâ€™, options)); the folder name in this case is set uber.com but depending on which target you look at it may be different. Once youâ€™ve done that you can simply run the script using node yourname.js. This will create a webserver listening on Port 3000 with an endpoint called /photos. So to access this you simply type: [http://yourserverip:3000/photos](http://yourserverip:3000/photos) to get a nice overview of the subdomains you have enumerated

> **System Tools**  

> apt update && apt upgrade  

> curl -sL [https://deb.nodesource.com/setup\_6.x](https://deb.nodesource.com/setup_6.x) | sudo -E bash -  

> apt install -y git wget python python-pip phantomjs xvfb screen slurm gem phantomjs imagemagick graphicsmagick nodejs

**Requirements for WebScreenshot**

> pip install webscreenshot  

> pip install selenium

**Requirements for express-photo-gallery**

> sudo npm install -g npm  

> npm install express-photo-gallery  

> npm install express  

> npm install -g epg-prep

**express-photo-gallery Script**

> JavaScript  

> var express = require(â€˜expressâ€™);  

> var app = express();

> 

> var Gallery = require(â€˜express-photo-galleryâ€™);

> 

> var options = {  

> title: â€˜My Awesome Photo Galleryâ€™  

> };

> 

> app.use(â€˜/photosâ€™, Gallery(â€˜uber.comâ€™, options));

> 

> app.listen(3000);

**Check CMS**

1\. Wappalyzer browser extension

2\. Builtwith â€” [https://builtwith.com/](https://builtwith.com/)

3\. Retire.js for old JS library

**WAF**

Look out for WAFs, you can use WafW00f for that  

[https://github.com/sandrogauci/wafw00f](https://github.com/sandrogauci/wafw00f)

Popular Google Dorks Use(finding Bug Bounty Websites)

> site:.eu responsible disclosure  

> inurl:index.php?id=  

> site:.nl bug bounty  

> â€œindex ofâ€ inurl:wp-content/ (Identify Wordpress Website)  

> inurl:â€q=user/passwordâ€ (for finding drupal cms )

All below are taken from 

[Prateek Tiwari](https://medium.com/u/dd4cbb5a8d44?source=post_page-----48aafbb8987--------------------------------)

 article:

> site:codepad.co â€œcompanyâ€  

> site:scribd.com â€œkeywordâ€  

> site:npmjs.com â€œkeywordâ€  

> site:npm.runkit.com â€œkeywordâ€  

> site:libraries.io â€œkeywordâ€  

> site:ycombinator.com â€œkeywordâ€  

> site:coggle.it â€œkeywordâ€  

> site:papaly.com â€œkeywordâ€  

> site:google.com â€œkeywordâ€  

> site:trello.com â€œkeywordâ€  

> site:prezi.com â€œkeywordâ€  

> site:jsdelivr.net â€œkeywordâ€  

> site:codepen.io â€œkeywordâ€  

> site:codeshare.io â€œkeywordâ€  

> site:sharecode.io â€œkeywordâ€  

> site:pastebin.com â€œkeywordâ€  

> site:replt.it â€œkeywordâ€  

> site:productforums..google.com â€œkeywordâ€  

> site:gitter.im â€œkeywordâ€  

> site:bitbucket.org â€œkeywordâ€  

> site:\*.atlassian.net â€œkeywordâ€  

> site:gitlab â€œkeywordâ€

**Wordlists/Payloads**

[*raft-large-words.txt*](https://github.com/danielmiessler/SecLists/blob/master/Discovery/Web-Content/raft-large-words.txt), [https://github.com/danielmiessler/SecLists](https://github.com/danielmiessler/SecLists)

content*discovery*all.txt from jhaddix: [https://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10](https://gist.github.com/jhaddix/b80ea67d85c13206125806f0828f4d10)

all.txt from jhaddix â€” [https://gist.github.com/jhaddix/f64c97d0863a78454e44c2f7119c2a6a](https://gist.github.com/jhaddix/f64c97d0863a78454e44c2f7119c2a6a)

PayloadAllTheThings â€” [https://github.com/swisskyrepo/PayloadsAllTheThings](https://github.com/swisskyrepo/PayloadsAllTheThings)

XSS Payloads- [http://www.xss-payloads.com/](http://www.xss-payloads.com/)  

XSS Payloads â€” [https://github.com/Pgaijin66/XSS-Payloads/blob/master/payload.txt](https://github.com/Pgaijin66/XSS-Payloads/blob/master/payload.txt)  

SQL Injection Payloads â€” [https://github.com/trietptm/SQL-Injection-Payloads](https://github.com/trietptm/SQL-Injection-Payloads)  

Google-Dorks Payloads â€” [https://gist.github.com/clarketm/919457847cece7ce40323dc217623054](https://gist.github.com/clarketm/919457847cece7ce40323dc217623054)

**Extracting vhosts**

Web Tool â€” [https://pentest-tools.com/information-gathering/find-virtual-hosts](https://pentest-tools.com/information-gathering/find-virtual-hosts)  

Virtual host scanner â€” [https://github.com/jobertabma/virtual-host-discovery](https://github.com/jobertabma/virtual-host-discovery)

git clone [https://github.com/jobertabma/virtual-host-discovery.git](https://github.com/jobertabma/virtual-host-discovery.git)  

ruby scan.rb â€” ip=192.168.1.101 â€” host=domain.tld

**Port Scan**

â€¢ Scan each individual IP address associated with their subdomains and having the output saved to a file

â€¢ Look for any services running on unusual ports or any service running on default ports which could be vulnerable (FTP, SSH, etc). Look for the version info on services running in order to determine whether anything is outdated and potentially vulnerable

1. **Masscan** : [https://github.com/robertdavidgraham/masscan](https://github.com/robertdavidgraham/masscan)

This is an Internet-scale port scanner. It can scan the entire Internet in under 6 minutes, transmitting 10 million packets per second, from a single machine.

**Setup:**

> $ sudo apt-get install git gcc make libpcap-dev  

> $ git clone [https://github.com/robertdavidgraham/masscan](https://github.com/robertdavidgraham/masscan)  

> $ cd masscan  

> $ make -j8

This puts the program in the masscan/bin subdirectory. Youâ€™ll have to manually copy it to something like /usr/local/bin if you want to install it elsewhere on the system.

**Usage:**

Shell script to run *dig*

â€¢ Because Masscan takes only IPs as input, not DNS names

â€¢ Use it to run Masscan against either a name domain or an IP range

> #!/bin/bash  

> strip=$(echo $1|sed â€˜s/https\\?:\\/\\///â€™)  

> echo â€œâ€  

> echo â€œ##################################################â€  

> host $strip  

> echo â€œ##################################################â€  

> echo â€œâ€  

> masscan -p1â€“65535 $(dig +short $strip|grep -oE â€œ\\b(\[0â€“9\]{1,3}\\.){3}\[0â€“9\]{1,3}\\bâ€|head -1) â€” max-rate 1000 |& tee $strip\_scan

> 

> Usage: masscan -p1â€“65535 -iL $TARGET\_LIST â€” max-rate 10000 -oG $TARGET\_OUTPUT  

> \# masscan -p80,8000â€“8100 10.0.0.0/8  

> \# masscan 10.0.0.0/8 -p80 â€” banners â€” source-ip 192.168.1.200

1. **Nmap**: [https://nmap.org/book/man.html](https://nmap.org/book/man.html)

**Github For Recon**

â€¢ Github is extremely helpful in finding Sensitive information regarding the targets. Access-keys, password, open endings, s3 buckets, backup files, etc. can be found on public GitHub repositories.

â€¢ Look for below things during a general first assessment(taken from edoverflow):

â€“ API and key. (Get some more endpoints and find API keys.)

â€“ token

â€“ secret

â€“ TODO

â€“ password

â€“ vulnerable ğŸ˜œ

â€“ http:// & https://

Then I will focus on terms that make me smile when developers mess things up:

â€“ CSRF

â€“ random

â€“ hash

â€“ MD5, SHA-1, SHA-2, etc.

â€“ HMAC

Github Recon Tools

1. **gitrob:** [https://github.com/michenriksen/gitrob](https://github.com/michenriksen/gitrob)

â€“ Gitrob is a tool to help find potentially sensitive files pushed to public repositories on Github. Gitrob will clone repositories belonging to a user or organization down to a configurable depth and iterate through the commit history and flag files that match signatures for potentially sensitive files. The findings will be presented through a web interface for easy browsing and analysis.

**Setup:**

> $ go get github.com/michenriksen/gitrob

**Usage:**

gitrob \[options\] target \[target2\] â€¦ \[targetN\]

1. **shhgit â€” **[https://github.com/eth0izzle/shhgit](https://github.com/eth0izzle/shhgit)

â€“ Shhgit finds secrets and sensitive files across GitHub code and Gists committed in near real time by listening to the GitHub Events API.

**Setup:**

> $ go get github.com/eth0izzle/shhgit

**Usage:**

â€¢ To configure it check the github page.

â€¢ Unlike other tools, you donâ€™t need to pass any targets with shhgit. Simply run $ shhgit to start watching GitHub commits and find secrets or sensitive files matching the included 120 signatures.

Alternatively, you can forgo the signatures and use shhgit with a search query, e.g. to find all AWS keys you could use

> shhgit â€” search-query AWS\_ACCESS\_KEY\_ID=AKIA

2. **Trufflehog:** [https://github.com/dxa4481/truffleHog](https://github.com/dxa4481/truffleHog)

â€“ Searches through git repositories for high entropy strings and secrets, digging deep into commit history.

**Setup:**

> pip install truffleHog

**Usage:**

> $ truffleHog â€” regex â€” entropy=False [https://github.com/dxa4481/truffleHog.git](https://github.com/dxa4481/truffleHog.git)

3. **git-all-secrets â€” **[https://github.com/anshumanbh/git-all-secrets](https://github.com/anshumanbh/git-all-secrets)

â€“ It clones public/private github repo of an org and user belonging to org and scan them.

â€“ Clones gist belonging to org and users of org.

**Setup:**

> git clone [https://github.com/anshumanbh/git-all-secrets.git](https://github.com/anshumanbh/git-all-secrets.git)

**Usage:**

> docker run â€” rm -it abhartiya/tools\_gitallsecrets â€” help  

> docker run -it abhartiya/tools\_gitallsecrets -token=<> -org=<>

4. **gitGraber **â€” [https://github.com/hisxo/gitGraber](https://github.com/hisxo/gitGraber)

â€“ monitor GitHub to search and find sensitive data in real time for different online services such as: Google, Amazon, Paypal, Github, Mailgun, Facebook, Twitter, Heroku, Stripe.

**Setup:**

> git clone [https://github.com/hisxo/gitGraber.git](https://github.com/hisxo/gitGraber.git)  

> cd gitGraber  

> pip3 install -r requirements.txt

**Usage:**

> python3 gitGraber.py -k wordlists/keywords.txt -q â€œuberâ€ -s

*We recommend creating a cron that will execute the script regulary* :

> \*/15 \* \* \* \* cd /BugBounty/gitGraber/ && /usr/bin/python3 gitGraber.py -k wordlists/keywords.txt -q â€œuberâ€ -s >/dev/null 2>&1

***Do it manually:***

â€¢ A quick Google â€œGratipay GitHubâ€ should return Gratipayâ€™s org page on GitHub. Then from there I am going to check what repos actually belong to the org and which are forked. You can do this by selecting the Type: dropdown on the right hand side of the page. Set it to Sources.

â€¢ Now, I am going to take a look at the different languages that the projects are written in. My favourite language is Python so I might start focusing on Python projects, but for recon I will mostly just keep note of the different languages.

â€¢ After that I will start using the GitHub search bar to look for specific keywords.

org:gratipay hmac

â€¢ There are 4 main sections to look out for here.

â€“ Repositories is nice for dedicated projects related to the keyword. For example, if the keyword is â€œpassword managerâ€, I might find they are building a password manager.

â€“ Code is the big one. You can search for classic lines of code that cause security vulnerabilities across the whole organization.

â€“ Commits is not usually my favourite area to look at manually, but if I see a low number I might have a quick look.

â€“ Issues this is the second biggest and will help you all with your recon. **This is the gold mine.**

Companies share so much information about their infrastructure in issue discussions and debates. Look for domains and subdomains in those tickets.

Chris: â€œOh, hey John. We forgot to add this certificate to this domain: vuln.example.com.â€

*noted*

â€¢ â€œcompany.comâ€ â€œdevâ€

â€¢ â€œdev.company.comâ€

â€¢ â€œcompany.comâ€ API\_key

â€¢ â€œcompany.comâ€ password

â€¢ â€œapi.company.comâ€ authorization

â€¢ others

**Read every JS** File

Sometimes, Javascript files contain sensitive information including various secrets or hardcoded tokens. Itâ€™s always worth to examine JS files manually.  

Find following things in Javascript.

â€¢ AWS or Other services Access keys

â€¢ AWS S3 buckets or other data storage buckets with read/write permissions.

â€¢ Open backup sql database endpoints

â€¢ Open endpoints of internal services.

JS File Parsing

1. **JSParser**: [https://github.com/nahamsec/JSParser](https://github.com/nahamsec/JSParser)

**Setup:**

> $ git clone [https://github.com/nahamsec/JSParser.git](https://github.com/nahamsec/JSParser.git)

$ `apt install libcurl4-openssl-dev libssl-dev`

> $ pip3 install -r requirements.txt  

> $ python setup.py install

**Usage:**

Run handler.py and then visit [*http://localhost:8008*](http://localhost:8008/).

> $ python handler.py

1. **LinkFinder:** [https://github.com/GerbenJavado/LinkFinder](https://github.com/GerbenJavado/LinkFinder)

LinkFinder is a python script written to discover endpoints and their parameters in JavaScript files

**Setup:**

> $ git clone [https://github.com/GerbenJavado/LinkFinder.git](https://github.com/GerbenJavado/LinkFinder.git)  

> $ cd LinkFinder  

> $ pip3 install -r requirements.txt  

> $ python setup.py install

**Usage:**

â€¢ Most basic usage to find endpoints in an online JavaScript file and output the HTML results to results.html:

> python linkfinder.py -i [https://example.com/1.js](https://example.com/1.js) -o results.html

â€¢ CLI/STDOUT output (doesnâ€™t use jsbeautifier, which makes it very fast):

> python linkfinder.py -i [https://example.com/1.js](https://example.com/1.js) -o cli

â€¢ Analyzing an entire domain and its JS files:

> python linkfinder.py -i [https://example.com](https://example.com/) -d

â€¢ Burp input (select in target the files you want to save, right click, Save selected items, feed that file as input):

> python linkfinder.py -i burpfile -b

â€¢ Enumerating an entire folder for JavaScript files, while looking for endpoints starting with /api/ and finally saving the results to results.html:

> python linkfinder.py -i â€˜Desktop/\*.jsâ€™ -r ^/api/ -o results.html

1. **getJS **â€” [https://github.com/003random/getJS](https://github.com/003random/getJS)

â€“ A tool to fastly get all javascript sources/files

**Setup:**

> go get github.com/003random/getJS

**Usage:**

> cat domains.txt | getJS |tojson

To feed urls from a file use:

> $ getJS -input=domains.txt

2. **InputScanner **â€” [https://github.com/zseano/InputScanner](https://github.com/zseano/InputScanner)

â€“ A tool designed to scrape a list of URLs and scrape input names (id if no name is found). This tool will also scrape .js urls found on each page (for further testing).

**Setup:**

Somewhere to run PHP. Recommended to use LAMP/XAMPP locally so you can just run the PHP from your computer locally. You can grab XAMPP from here: [https://www.apachefriends.org/index.html.](https://www.apachefriends.org/index.html.)

â€¢ Clone in /var/www

> git clone [https://github.com/zseano/InputScanner.git](https://github.com/zseano/InputScanner.git)

**Usage:**

â€“ Now youâ€™re setup, itâ€™s time to gather some URLs to test. Use Burp Spider to crawl.

â€“ Once the spider has finished (or you stop it), right click on the host and click â€œCopy Urls in this hostâ€.

â€“ Once copied, paste them into urls.txt. Now open payloads.txt and enter some payloads you wish to inject into each parameter (such as xssâ€ xssâ€™ to test for the reflection of â€œ and â€˜ characters on iputs. This will help automate looking for XSS). This script will inject each payload into each parameter.. so the more payloads, the more requests youâ€™ll be sending.

â€“ Now visit [http://127.0.0.1/InputScanner/](http://127.0.0.1/InputScanner/) and begin scan

â€“ Once the scanner is complete you will be given 4 txt file outputs (see below). Use the BURP Intruder to import your lists and run through them.

â€“ 4 files are outputted in the /outputs/ folder: JS-output.txt, GET-output.txt, POSTHost-output.txt, POSTData-output.txt.

â€¢ GET-output.txt is a file which can be easily imported into a BURP intruder attack (using the Spider type). Set the position in the header (GET Â§valÂ§ HTTP/1.0) and run the attack. Make sure to play with settings and untick â€œURL-encode these charactersâ€, found on the Payloads tab. Currently the script will echo the HOST url, and I just mass-replace in a text editor such as Sublime. (Replace to null). You are free to modify the script for how you see fit.

â€¢ JS-output.xt contains a list of .js urls found on each page. The format is found@https://www.example.com/|https://www.example.com/eg.js|, and this is so you can easily load it into JS-Scan (another tool released by me) and it will let you know where each .js file was found as it scrapes.

â€¢ POSTHost-output.txt contains a list of HOST urls (such as [https://www.google.com/)](https://www.google.com/)) which is used for the â€œPitchforkâ€ burp intruder attack. Use this file along with POSTData-output.txt. Set attack type to â€œPitch forkâ€ and set one position in the header (same as Sniper attack above), and another at the bottom of the request (the post data sent). Make sure to set a Content-Length etc.

â€¢ POSTData-output.txt contains post data. (param1=xssâ€&param2=xssâ€&param3=xssâ€)

3. **JS-Scan **â€” [https://github.com/zseano/JS-Scan](https://github.com/zseano/JS-Scan)

â€“ A tool designed to scrape a list of .js files and extract urls, as well as juicy information.

**Setup:**

1\. Install LAMP/XAMPP Server.

2\. InputScanner to scrape .js files

3\. Clone repo:

> git clone [https://github.com/zseano/JS-Scan.git](https://github.com/zseano/JS-Scan.git)

**Usage:**

â€“ Import JS-output.txt file in this interface â€” [http://127.0.0.1/JS-Scan/](http://127.0.0.1/JS-Scan/)

**WaybackUrl**

â€¢ Searching for the targets webpages in waybackmachine, the following things can be found.  

Old and abandoned JS files.  

Old API endpoints.  

Abandoned CDNâ€™s Endpoints.  

Abandoned Subdomains.  

Dev & staging endpoint with juicy info in source code comments.  

If you are getting 403 on a page, you can also search that 403 pages of targets in way back machine sometimes, you will find them open with helpful information.

1. **waybackurls â€” **[https://github.com/tomnomnom/waybackurls](https://github.com/tomnomnom/waybackurls)

â€“ Fetch all the URLs that the Wayback Machine knows about for a domain.

**Setup**:

> go get github.com/tomnomnom/waybackurls

**Usage:**

> cat domains.txt | waybackurls > urls

1. **waybackunifier **â€” [https://github.com/mhmdiaa/waybackunifier](https://github.com/mhmdiaa/waybackunifier)

â€“ WaybackUnifier allows you to take a look at how a file has ever looked by aggregating all versions of this file, and creating a unified version that contains every line that has ever been in it.

**Setup:**

> go get github.com/mhmdiaa/waybackunifier

**Usage:**

Syntax:

\-concurrency int  

Number of requests to make in parallel (default 1)  

\-output string  

File to save results in (default â€œoutput.txtâ€)  

\-sub string  

list of comma-separated substrings to look for in snapshots (snapshots will only be considered if they contnain one of them) (default â€œDisallow,disallowâ€)  

\-url string  

URL to unify versions of (without protocol prefix) (default â€œsite.com/robots.txtâ€)

## Webmap

![](https://miro.medium.com/max/700/1*hQc2TTFMq6khSu9nKFncYA.png)

Lot of web mindmaps: [https://pentester.land/cheatsheets/2019/03/25/compilation-of-recon-workflows.html](https://pentester.land/cheatsheets/2019/03/25/compilation-of-recon-workflows.html)

## Subdomain TakeoverTools

1. **SubOver **â€” [https://github.com/Ice3man543/SubOver](https://github.com/Ice3man543/SubOver)

â€“ A Powerful Subdomain Takeover Tool

**Setup:**

> go get github.com/Ice3man543/SubOver

**Usage:**

> ./SubOver -l subdomains.txt

2. **subjack **â€” [https://github.com/haccer/subjack](https://github.com/haccer/subjack)

â€“ Subjack is a Subdomain Takeover tool written in Go designed to scan a list of subdomains concurrently and identify ones that are able to be hijacked

â€“ Subjack will also check for subdomains attached to domains that donâ€™t exist (NXDOMAIN) and are **available to be registered**. No need for dig ever again

**Setup:**

> go get github.com/haccer/subjack

**Usage:**

> ./subjack -w subdomains.txt -t 100 -timeout 30 -o results.txt -ssl

3. **TakeOver-v1 **â€” [https://github.com/samhaxr/TakeOver-v1](https://github.com/samhaxr/TakeOver-v1)

â€“ It gives the CNAME of all the subdomains from a file

**Setup:**

> git clone [https://github.com/samhaxr/TakeOver-v1.git](https://github.com/samhaxr/TakeOver-v1.git)

> 

> Usage:

> 

> ./takeover.sh subdomain.txt

4. **subzy **â€” [https://github.com/LukaSikic/subzy](https://github.com/LukaSikic/subzy)

â€¢ Subdomain takeover tool which works based on matching response fingerprings from [*can-i-take-over-xyz*](https://github.com/EdOverflow/can-i-take-over-xyz/blob/master/README.md)

**Setup:**

> go get -u -v github.com/lukasikic/subzy  

> go install -v github.com/lukasikic/subzy

**Usage:**

> List of subdomains  

>   

> ./subzy -targets list.txt  

> Single or few subdomains  

>   

> ./subzy -target test.google.com  

> ./subzy -target test.google.com,https://test.yahoo.com

## Other/Interesting Tools

1. **Parameth **â€” [https://github.com/maK-/parameth](https://github.com/maK-/parameth)

â€“ This tool can be used to brute discover GET and POST parameters

â€“ Often when you are busting a directory for common files, you can identify scripts (for example test.php) that look like they need to be passed an unknown parameter. This hopefully can help find them.

**Setup:**

> git clone [https://github.com/maK-/parameth.git](https://github.com/maK-/parameth.git)  

> virtualenv venv  

> . ./venv/bin/activate  

> pip install -u -r requirements.txt

**Usage:**

> ./parameth.py -u [http://example.com/test.php](http://example.com/test.php)

2. **Arjun **â€” [https://github.com/s0md3v/Arjun](https://github.com/s0md3v/Arjun)

â€“ HTTP parameter discovery suite.

**Setup:**

> [https://github.com/s0md3v/Arjun.git](https://github.com/s0md3v/Arjun.git)

**Usage:**

â€“ Scanning a single URL

To find GET parameters, you can simply do:

> python3 arjun.py -u [https://api.example.com/endpoint](https://api.example.com/endpoint) â€” get

Similarly, use â€” post for POST and â€” json to look for JSON parameters.

â€“ Scanning multiple URLs

A list of URLs stored in a file can be test by using the â€” urls option as follows

> python3 arjun.py â€” urls targets.txt â€” get

3. **fuxploitder **â€” [https://github.com/almandin/fuxploider](https://github.com/almandin/fuxploider)

â€“ File upload vulnerability scanner and exploitation tool.

**Setup:**

> git clone [https://github.com/almandin/fuxploider.git](https://github.com/almandin/fuxploider.git)  

> cd fuxploider  

> pip3 install -r requirements.txt

**Usage:**

To get a list of basic options and switches use :

> python3 fuxploider.py -h

Basic example :

> python3 fuxploider.py â€” url [https://awesomeFileUploadService.com](https://awesomefileuploadservice.com/) â€” not-regex â€œwrong file typeâ€

4. **Syborg â€” **[https://github.com/MilindPurswani/Syborg](https://github.com/MilindPurswani/Syborg)

â€“ Recursive DNS Subdomain Enumerator with dead-end avoidance system

**Setup:**

Clone the repo using the git clone command as follows:

> git clone [https://github.com/MilindPurswani/Syborg.git](https://github.com/MilindPurswani/Syborg.git)

Resolve the Dependencies:

> pip3 install -r requirements.txt

**Usage:**

> python3 syborg.py yahoo.com

**At times, it is also possible that Syborg will hit High CPU Usage and that can cost you a lot if you are trying to use this tool on your VPS. Therefore to limit that use another utility called Cpulimit**

> cpulimit -l 50 -p $(pgrep python3)

This tool can be downloaded as follows:

> sudo apt install cpulimit

5. **dnsgen **â€” [https://github.com/ProjectAnte/dnsgen](https://github.com/ProjectAnte/dnsgen)

â€“ Generates combination of domain names from the provided input.

â€“ Combinations are created based on wordlist. Custom words are extracted per execution.

**Setup:**

> pip3 install dnsgen

..or from GitHub:

> git clone [https://github.com/ProjectAnte/dnsgen](https://github.com/ProjectAnte/dnsgen)  

> cd dnsgen  

> pip3 install -r requirements.txt  

> python3 setup.py install

**Usage:**

> $ dnsgen domains.txt (domains.txt contains a list of active domain names)

**Combination with massdns:**

> $ cat domains.txt | dnsgen â€” | massdns -r /path/to/resolvers.txt -t A -o J â€” flush 2>/dev/null

6. **SSRFmap **â€” [https://github.com/swisskyrepo/SSRFmap](https://github.com/swisskyrepo/SSRFmap)

â€“ Automatic SSRF fuzzer and exploitation tool

â€“ SSRFmap takes a Burp request file as input and a parameter to fuzz.

**Setup:**

> $ git clone [https://github.com/swisskyrepo/SSRFmap](https://github.com/swisskyrepo/SSRFmap)  

> $ cd SSRFmap/  

> $ pip3 install -r requirements.txt

**Usage:**

First you need a request with a parameter to fuzz, Burp requests works well with SSRFmap. They should look like the following. More examples are available in the **/data** folder.

POST /ssrf HTTP/1.1  

Host: 127.0.0.1:5000  

User-Agent: Mozilla/5.0 (X11; Linux x86\_64; rv:62.0) Gecko/20100101 Firefox/62.0  

Accept: text/html,application/xhtml+xml,application/xml;q=0.9,\*/\*;q=0.8  

Accept-Language: en-US,en;q=0.5  

Accept-Encoding: gzip, deflate  

Referer: [http://mysimple.ssrf/](http://mysimple.ssrf/)  

Content-Type: application/x-www-form-urlencoded  

Content-Length: 31  

Connection: close  

Upgrade-Insecure-Requests: 1  

  

url=https%3A%2F%2Fwww.google.fr

Use the -m followed by module name (separated by a , if you want to launch several modules).

> \# Launch a portscan on localhost and read default files  

> python ssrfmap.py -r data/request.txt -p url -m readfiles,portscan

7. **nip.io **â€” [https://nip.io/](https://nip.io/)

â€“ Dead simple wildcard DNS for any IP Address

Stop editing your etc/hosts file with custom hostname and IP address mappings.

[*nip.io*](https://nip.io/) allows you to do that by mapping any IP Address to a hostname using the following formats:

â€“ 10.0.0.1.nip.io maps to 10.0.0.1

â€“ 192â€“168â€“1â€“250.nip.io maps to 192.168.1.250

8. **CORS Scanner **â€” [https://github.com/chenjj/CORScanner](https://github.com/chenjj/CORScanner)

â€“ Fast CORS misconfiguration vulnerabilities scanner

**Setup:**

> git clone [https://github.com/chenjj/CORScanner.git](https://github.com/chenjj/CORScanner.git)

â€“ Install dependencies

> sudo pip install -r requirements.txt

**Usage:**

â€“ To check CORS misconfigurations of specific domain:

> python cors\_scan.py -u example.com

â€“ To check CORS misconfigurations of specific URL:

> python cors\_scan.py -u [http://example.com/restapi](http://example.com/restapi)

â€“ To check CORS misconfiguration with specific headers:

> python cors\_scan.py -u example.com -d â€œCookie: testâ€

â€“ To check CORS misconfigurations of multiple domains/URLs:

> python cors\_scan.py -i top\_100\_domains.txt -t 100

9. **Blazy **â€” [https://github.com/s0md3v/Blazy](https://github.com/s0md3v/Blazy)

â€“ Blazy is a modern login bruteforcer which also tests for CSRF, Clickjacking, Cloudflare and WAF .

**Setup:**

> git clone [https://github.com/UltimateHackers/Blazy](https://github.com/UltimateHackers/Blazy)  

> cd Blazy  

> pip install -r requirements.txt

**Usage:**

> python blazy.py

10. **XSStrike **â€” [https://github.com/s0md3v/XSStrike](https://github.com/s0md3v/XSStrike)

â€“ Most advanced XSS scanner.

**Setup:**

> git clone [https://github.com/s0md3v/XSStrike.git](https://github.com/s0md3v/XSStrike.git)

**Usage:**

> Scan a single URL  

> Option: -u or â€” url  

>   

> Test a single webpage which uses GET method.  

>   

> python xsstrike.py -u â€œhttp://example.com/search.php?q=query"  

>   

> Supplying POST data  

> python xsstrike.py -u â€œhttp://example.com/search.php" â€” data â€œq=queryâ€

11. **Commix â€” **[**https://github.com/commixproject/commix**](https://github.com/commixproject/commix)

â€“ Automated All-in-One OS command injection and exploitation tool.

**Setup:**

> git clone [https://github.com/commixproject/commix.git](https://github.com/commixproject/commix.git) commix

**Usage:**

> [https://github.com/commixproject/commix/wiki/Usage-Examples](https://github.com/commixproject/commix/wiki/Usage-Examples)

> 

> \# python commix.py â€” url=â€http://192.168.178.58/DVWA-1.0.8/vulnerabilities/exec/#" â€” data=â€ip=127.0.0.1&Submit=submitâ€ â€” cookie=â€security=medium; PHPSESSID=nq30op434117mo7o2oe5bl7is4"

12. **Bolt **â€” [https://github.com/s0md3v/Bolt](https://github.com/s0md3v/Bolt)

â€“ A dumb CSRF scanner

**Setup:**

> git clone [https://github.com/s0md3v/Bolt.git](https://github.com/s0md3v/Bolt.git)

**Usage:**

Scanning a website for CSRF using Bolt is as easy as doing

> python3 bolt.py -u [https://github.com](https://github.com/) -l 2

1. **bass **â€” [https://github.com/Abss0x7tbh/bass](https://github.com/Abss0x7tbh/bass)

â€“ Bass grabs you those â€œextra resolversâ€ you are missing out on when performing Active DNS  

enumeration. Add anywhere from 100â€“6k resolvers to your â€œresolver.txtâ€

**Setup:**

> git clone [https://github.com/Abss0x7tbh/bass.git](https://github.com/Abss0x7tbh/bass.git)  

> cd bass  

> pip3 install -r requirements.txt

**Usage:**

> python3 bass.py -d target.com -o output/file/for/final\_resolver\_list.txt

1. **meg â€” **[https://github.com/tomnomnom/meg](https://github.com/tomnomnom/meg)

â€¢ meg is a tool for fetching lots of URLs but still being â€˜niceâ€™ to servers.

It can be used to fetch many paths for many hosts; fetching one path for all hosts before moving on to the next path and repeating.

**Setup:**

> go get -u github.com/tomnomnom/meg

**Usage:**

Given a file full of paths:

/robots.txt  

/.well-known/security.txt  

/package.json

And a file full of hosts (with a protocol):

[http://example.com](http://example.com/)  

[https://example.com](https://example.com/)  

[http://example.net](http://example.net/)

meg will request each *path* for every *host*:

â–¶ meg â€” verbose paths hosts

> meg <endpoint> <host>  

> $ meg / [https://edoverflow.com](https://edoverflow.com/)

The latter command requests the top-level directory for [https://edoverflow.com](https://edoverflow.com/) (https://edoverflow.com/). It is important to note, that protocols most be specified; meg does not automatically prefix hosts. If you happen to have a list of targets without protocols, make sure to sed the file and add the correct protocol.

> $ sed â€˜s#^#http://#g' list-of-hosts > output

By default meg stores all output in an out/ directory, but if you would like to include a dedicated output directory, all it takes is appending the output directory to your command as follows:

> $ meg / [https://edoverflow.com](https://edoverflow.com/) out-edoverflow/

Say we want to pinpoint specific files that could either assist us further while targeting a platform or be an actual security issue in itself if exposed to the public, all it takes is a list of endpoints (lists/php) and a series of targets (targets-all). For this process, storing all pages that return a â€œ200 OKâ€ status code will help us sieve out most of the noise and false-positives (-s 200).

> $ meg -s 200 \\  

> lists/php targets-all \\  

> out-php/ 2> /dev/null

2. **tojson** â€” [https://github.com/tomnomnom/hacks/tree/master/tojson](https://github.com/tomnomnom/hacks/tree/master/tojson)

â€¢ Turn lines of stdin into JSON.

**Setup:**

> go get -u github.com/tomnomnom/hacks/tojson

**Usage:**

> getJS -url=https://poc-server.com | tojson  

> ls -l | tojson

3. **interlace **â€” [https://github.com/codingo/Interlace](https://github.com/codingo/Interlace)

â€¢ Easily turn single threaded command line applications into a fast, multi-threaded application with CIDR and glob support.

**Setup:**

> $ git clone [https://github.com/codingo/Interlace.git](https://github.com/codingo/Interlace.git)  

> $ python3 setup.py install

**Usage:**

Letâ€™s say we need to run Nikto (a basic, free web server vulnerability scanner) over a list of hosts:

luke$ cat targets.txt  

hackerone.com  

bugcrowd.com  

yahoo.com  

google.com

$ interlace -tL ./targets.txt -threads 5 -c â€œnikto â€” host \_target\_ > ./\_target\_-nikto.txtâ€ -v

\==============================================Interlace v1.2 by Michael Skelton (@codingo\_)==============================================\[13:06:16\] \[VERBOSE\] \[nikto â€” host yahoo.com > ./yahoo.com-nikto.txt\] Added after processing\[13:06:16\] \[VERBOSE\] \[nikto â€” host google.com > ./google.com-nikto.txt\] Added after processing\[13:06:16\] \[VERBOSE\] \[nikto â€” host hackerone.com > ./hackerone.com-nikto.txt\] Added after processing\[13:06:16\] \[VERBOSE\] \[nikto â€” host bugcrowd.com > ./bugcrowd.com-nikto.txt\] Added after processing\[13:06:16\] \[THREAD\] \[nikto â€” host google.com > ./google.com-nikto.txt\] Added to Queue

Letâ€™s break this down a bit â€” hereâ€™s the command I ran:

interlace -tL ./targets.txt -threads 5 -c â€œnikto â€” host \_target\_ > ./\_target\_-nikto.txtâ€ -v

â€“ interlace is the name of the tool.

â€“ -tL ./targets.txt defines a file with a list of hosts.

â€“ -threads 5 defines the number of threads.

â€“ -c should be immediately followed by the command you want to run.

â€“ â€œnikto â€” host \_target\_ > ./\_target\_-nikto.txtâ€ is the actual command which will be run, note that instances of \_target\_ will be replaced with each line in the ./targets.txt file.

â€“ -v makes it verbose.

## Good Articles to Read

1\. Subdomain Takeover by Patrik â€” [https://0xpatrik.com/subdomain-takeover/,](https://0xpatrik.com/subdomain-takeover/,) [https://0xpatrik.com/takeover-proofs/](https://0xpatrik.com/takeover-proofs/)

2\. Subdomain Enumeration â€” [https://0xpatrik.com/subdomain-enumeration-smarter/,](https://0xpatrik.com/subdomain-enumeration-smarter/,) [https://0xpatrik.com/subdomain-enumeration-2019/](https://0xpatrik.com/subdomain-enumeration-2019/)

3\. Can-I-take-over-xyz â€” [https://github.com/EdOverflow/can-i-take-over-xyz](https://github.com/EdOverflow/can-i-take-over-xyz)

4\. File Upload XSS â€” [https://brutelogic.com.br/blog/file-upload-xss/](https://brutelogic.com.br/blog/file-upload-xss/)

5\. Serverless Toolkit for Pentesters â€” [https://blog.ropnop.com/serverless-toolkit-for-pentesters/](https://blog.ropnop.com/serverless-toolkit-for-pentesters/)

6\. Docker for Pentesters â€” [https://blog.ropnop.com/docker-for-pentesters/](https://blog.ropnop.com/docker-for-pentesters/)

7\. For coding â€” [https://learnxinyminutes.com/](https://learnxinyminutes.com/)

8\. Android Security Lab Setup â€” [https://medium.com/@ehsahil/basic-android-security-testing-lab-part-1-a2b87e667533](https://medium.com/@ehsahil/basic-android-security-testing-lab-part-1-a2b87e667533)

9\. SSL Bypass â€” [https://medium.com/@ved\_wayal/hail-frida-the-universal-ssl-pinning-bypass-for-android-e9e1d733d29](https://medium.com/@ved_wayal/hail-frida-the-universal-ssl-pinning-bypass-for-android-e9e1d733d29)

10\. Bypass Certificate Pinning â€” [*https://blog.it-securityguard.com/the-stony-path-of-android-%F0%9F%A4%96-bug-bounty-bypassing-certificate-pinning/*](https://blog.it-securityguard.com/the-stony-path-of-android-%F0%9F%A4%96-bug-bounty-bypassing-certificate-pinning/)

11\. Burp macros and Session handling â€” [https://digi.ninja/blog/burp\_macros.php](https://digi.ninja/blog/burp_macros.php)

12\. Burp Extensions â€” [https://blog.usejournal.com/bug-hunting-methodology-part-2-5579dac06150](https://blog.usejournal.com/bug-hunting-methodology-part-2-5579dac06150)

13\. JSON CSRF to form data attack â€” [https://medium.com/@osamaavvan/json-csrf-to-formdata-attack-eb65272376a2](https://medium.com/@osamaavvan/json-csrf-to-formdata-attack-eb65272376a2)

14\. meg â€” [https://edoverflow.com/2018/meg/](https://edoverflow.com/2018/meg/)

15\. assetnote: [https://github.com/tdr130/assetnote](https://github.com/tdr130/assetnote) Push notifications for the new domain

16\. interlace : [https://medium.com/@hakluke/interlace-a-productivity-tool-for-pentesters-and-bug-hunters-automate-and-multithread-your-d18c81371d3d](https://medium.com/@hakluke/interlace-a-productivity-tool-for-pentesters-and-bug-hunters-automate-and-multithread-your-d18c81371d3d)

17\. http-desync-attacks-request-smuggling-reborn- [https://portswigger.net/research/http-desync-attacks-request-smuggling-reborn](https://portswigger.net/research/http-desync-attacks-request-smuggling-reborn)

## Scripts

â€¢ the art of subdomain enumeration â€” [https://github.com/appsecco/the-art-of-subdomain-enumeration](https://github.com/appsecco/the-art-of-subdomain-enumeration)

â€¢ Setup Bug Bounty tools : [https://gist.github.com/LuD1161/66f30da6d8b6c1c05b9f6708525ea885](https://gist.github.com/LuD1161/66f30da6d8b6c1c05b9f6708525ea885)

â€¢ ReconPi â€” [https://github.com/x1mdev/ReconPi/tree/dev/v2.0](https://github.com/x1mdev/ReconPi/tree/dev/v2.0)

â€¢ TotalRecon â€” Insalls all the tools â€” [https://github.com/vitalysim/totalrecon](https://github.com/vitalysim/totalrecon)

â€¢ Auto Recon Bash Script â€” [https://github.com/mehulpanchal007/venom](https://github.com/mehulpanchal007/venom)

## Recon My Way

1\. Do Subdomain enumeration using amass, assetfinder, subfind

> amass enum â€” passive -d <DOMAIN>

> 

> assetfinder â€” subs-only <domain>

> 

> subfinder -d freelancer.com

> 

> subfinder -d <domain> -recursive -silent -t 200 -v -o <outfile>

2\. Use commonspeak2 wordlist to get probable permutations of above subdomains.

[https://github.com/assetnote/commonspeak2-wordlists](https://github.com/assetnote/commonspeak2-wordlists)

To generate the possibilities, you can use this simple Python snippet:

scope = â€˜<DOMAIN>â€™  

wordlist = open(â€˜./commonspeak2.txtâ€™).read().split(â€˜\\nâ€™)  

  

for word in wordlist:  

if not word.strip():  

continue  

print(â€˜{}.{}\\nâ€™.format(word.strip(), scope))

3\. Use massdns to resolve all the above domains:

./bin/massdns -r lists/resolvers.txt -t A domains.txt > results.txt

4\. To get the best resolvers.txt use bass tool:

python3 bass.py -d target.com -o output/file/for/final\_resolver\_list.txt

5\. Use dnsgen to generates combination of domain names from the provided input.

cat domains.txt | dnsgen â€” | massdns -r /path/to/resolvers.txt -t A -o J â€” flush 2>/dev/null

6\. Port Scan Using masscan and nmap for version scan

Shell script to run *dig*

â€“ Because Masscan takes only IPs as input, not DNS names

â€“ Use it to run Masscan against either a name domain or an IP range

#!/bin/bash  

strip=$(echo $1|sed â€˜s/https\\?:\\/\\///â€™)  

echo â€œâ€  

echo â€œ##################################################â€  

host $strip  

echo â€œ##################################################â€  

echo â€œâ€  

masscan -p1â€“65535 $(dig +short $strip|grep -oE â€œ\\b(\[0â€“9\]{1,3}\\.){3}\[0â€“9\]{1,3}\\bâ€|head -1) â€” max-rate 1000 |& tee $strip\_scan

or

nmap -iL list.txt -Pn -n -sn -oG output.txt  

masscan -iL output.txt -p 0â€“65535 â€” max-rate 1000

or run massscan + nmap using below script

wget [https://raw.githubusercontent.com/PacktPublishing/Mastering-Kali-Linux-for-Advanced-Penetration-Testing-Third-Edition/master/Chapter03/massNmap.sh](https://raw.githubusercontent.com/PacktPublishing/Mastering-Kali-Linux-for-Advanced-Penetration-Testing-Third-Edition/master/Chapter03/massNmap.sh)

**nmap scan with output in an nice xml file** !  

$ sudo nmap -sS -T4 -sC -oA my*report*name â€” stylesheet [*https://raw.githubusercontent.com/honze-net/nmap-bootstrap-xsl/master/nmap-bootstrap.xsl*](https://t.co/DYeG9mx6kT?amp=1) -iL subdomain.txt

7\. Do github recon

8\. Take screenshot using aquatone.

cat hosts.txt | aquatone -out ~/aquatone/example.com

9\. Run ffuz or gobuster to directory bruteforce/content discovery on a particular domain/subdomain

ffuf -w /path/to/wordlist -u [https://target/FUZZ](https://target/FUZZ)

10\. Read JS file, get the endpoints, check if there is any secret token/key in JS files.

11\. Use waybackurls to get old JS files, and 403 files.

â€“ Generate wordlist using wayback

\# curl -s â€œhttp://web.archive.org/cdx/search/cdx?url=hackerone.com/\*&output=text&fl=original&collapse=urlkey" | sed â€˜s/\\//\\n/gâ€™ | sort -u | grep -v â€˜svg\\|.png\\|.img\\|.ttf\\|http:\\|:\\|.eot\\|woff\\|ico\\|css\\|bootstrap\\|wordpress\\|.jpg\\|.jpegâ€™ > wordlist

or

\# curl -L [http://xrl.us/installperlnix](http://xrl.us/installperlnix) | bash  

\# curl -s â€œhttp://web.archive.org/cdx/search/cdx?url=hackerone.com/\*&output=text&fl=original&collapse=urlkey" | sed â€˜s/\\//\\n/gâ€™ | sort -u | grep -v â€˜svg\\|.png\\|.img\\|.ttf\\|http:\\|:\\|.eot\\|woff\\|ico\\|css\\|bootstrap\\|wordpress\\|.jpg\\|.jpegâ€™ | perl -pe â€˜s/\\%(\\w\\w)/chr hex $1/geâ€™ > wordlist.txt

12\. One liner to import whole list of subdomains into Burp suite for automated scanning!

> cat <file-name> | parallel -j 200 curl -L -o /dev/null {} -x 127.0.0.1:8080 -k -s

![](https://miro.medium.com/max/700/1*DJduIGe7WptA4YYJ3T8RNQ.png)

![](https://miro.medium.com/max/700/1*wBFKMDuCX_x1sNXGDKiK9A.png)
